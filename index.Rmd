---
title: "Relaxing/Calming aspects of music for humans and dogs"
author: "Jan Hengeveld"
output: 
  flexdashboard::flex_dashboard:
    storyboard: true
    theme:
      primary: "#f59842"
      navbar_bg: "#f5b042"
---

<style type="text/css">
  body{
  font-size: 12pt;
}
</style>


```{r, setup}

library(tidyverse)
library(plotly)
library(spotifyr)
library(compmus)
library(grid)
library(knitr)
library(gridExtra)
library(heatmaply)
library(ggdendro)
library(recipes)
```

```{r,}
circshift <- function(v, n) {
  if (n == 0) v else c(tail(v, n), head(v, -n))
}

#      C     C#    D     Eb    E     F     F#    G     Ab    A     Bb    B
major_chord <-
  c(   1,    0,    0,    0,    1,    0,    0,    1,    0,    0,    0,    0)
minor_chord <-
  c(   1,    0,    0,    1,    0,    0,    0,    1,    0,    0,    0,    0)
seventh_chord <-
  c(   1,    0,    0,    0,    1,    0,    0,    1,    0,    0,    1,    0)

major_key <-
  c(6.35, 2.23, 3.48, 2.33, 4.38, 4.09, 2.52, 5.19, 2.39, 3.66, 2.29, 2.88)
minor_key <-
  c(6.33, 2.68, 3.52, 5.38, 2.60, 3.53, 2.54, 4.75, 3.98, 2.69, 3.34, 3.17)

chord_templates <-
  tribble(
    ~name, ~template,
    "Gb:7", circshift(seventh_chord, 6),
    "Gb:maj", circshift(major_chord, 6),
    "Bb:min", circshift(minor_chord, 10),
    "Db:maj", circshift(major_chord, 1),
    "F:min", circshift(minor_chord, 5),
    "Ab:7", circshift(seventh_chord, 8),
    "Ab:maj", circshift(major_chord, 8),
    "C:min", circshift(minor_chord, 0),
    "Eb:7", circshift(seventh_chord, 3),
    "Eb:maj", circshift(major_chord, 3),
    "G:min", circshift(minor_chord, 7),
    "Bb:7", circshift(seventh_chord, 10),
    "Bb:maj", circshift(major_chord, 10),
    "D:min", circshift(minor_chord, 2),
    "F:7", circshift(seventh_chord, 5),
    "F:maj", circshift(major_chord, 5),
    "A:min", circshift(minor_chord, 9),
    "C:7", circshift(seventh_chord, 0),
    "C:maj", circshift(major_chord, 0),
    "E:min", circshift(minor_chord, 4),
    "G:7", circshift(seventh_chord, 7),
    "G:maj", circshift(major_chord, 7),
    "B:min", circshift(minor_chord, 11),
    "D:7", circshift(seventh_chord, 2),
    "D:maj", circshift(major_chord, 2),
    "F#:min", circshift(minor_chord, 6),
    "A:7", circshift(seventh_chord, 9),
    "A:maj", circshift(major_chord, 9),
    "C#:min", circshift(minor_chord, 1),
    "E:7", circshift(seventh_chord, 4),
    "E:maj", circshift(major_chord, 4),
    "G#:min", circshift(minor_chord, 8),
    "B:7", circshift(seventh_chord, 11),
    "B:maj", circshift(major_chord, 11),
    "D#:min", circshift(minor_chord, 3)
  )

key_templates <-
  tribble(
    ~name, ~template,
    "Gb:maj", circshift(major_key, 6),
    "Bb:min", circshift(minor_key, 10),
    "Db:maj", circshift(major_key, 1),
    "F:min", circshift(minor_key, 5),
    "Ab:maj", circshift(major_key, 8),
    "C:min", circshift(minor_key, 0),
    "Eb:maj", circshift(major_key, 3),
    "G:min", circshift(minor_key, 7),
    "Bb:maj", circshift(major_key, 10),
    "D:min", circshift(minor_key, 2),
    "F:maj", circshift(major_key, 5),
    "A:min", circshift(minor_key, 9),
    "C:maj", circshift(major_key, 0),
    "E:min", circshift(minor_key, 4),
    "G:maj", circshift(major_key, 7),
    "B:min", circshift(minor_key, 11),
    "D:maj", circshift(major_key, 2),
    "F#:min", circshift(minor_key, 6),
    "A:maj", circshift(major_key, 9),
    "C#:min", circshift(minor_key, 1),
    "E:maj", circshift(major_key, 4),
    "G#:min", circshift(minor_key, 8),
    "B:maj", circshift(major_key, 11),
    "D#:min", circshift(minor_key, 3)
  )
```

```{r, selecting data from spotify and combining for analysis}

dogs <- get_playlist_audio_features("", "5hQo2asoxqQrnJFeufycj1")
humans <- get_playlist_audio_features("", "3B0FtfxNiFOo82o8lmJcIp")
dogsscience <- get_playlist_audio_features("", "0km3mDUsP3LYDS1BZfqsY5")
humansscience <- get_playlist_audio_features("", "1t06IDDtn5eYo4Ow7Fwmcb")

combine4lists <-
bind_rows(
humans |> mutate(category = "Humans"),
dogs |> mutate(category = "Dogs"),
dogsscience |> mutate(category = "Dogsscience"),
humansscience |> mutate(category = "Humansscience")
)
```
### Tempograms and novelty - Homework week 11; a boxplot on tempo for my 4 playlists

```{r}
plot1 <- combine4lists |>
 ggplot(aes(x = category, y = tempo)) +
  geom_boxplot()
grid.arrange(plot1)
```


```{r, 1 single variable compared; tempo plots and gram  sjwsjs}

plot1 <- combine4lists |>
 ggplot(aes(x = category, y = tempo)) +
  geom_boxplot()

singlehumanscience <- get_tidy_audio_analysis("6kkwzB6hXLIONkEk9JciA6")

tempogramsfalseforweightless <- singlehumanscience |>
  tempogram(window_size = 4, hop_size = 2, bpms = 40:160, cyclic = FALSE) |>
 ggplot(aes(x = time, y = bpm, fill = power)) +
  geom_raster() +
  scale_fill_viridis_c(guide = "none") +
  labs(x = "Time (s)", title = "Weightless - Marconi Union", y = "Tempo (BPM)") +
  theme_classic()

tempogramstrueforweightless <- singlehumanscience |>
  tempogram(window_size = 4, hop_size = 2, bpms = 40:80, cyclic = TRUE) |>
  ggplot(aes(x = time, y = bpm, fill = power)) +
  geom_raster() +
  scale_fill_viridis_c(guide = "none") +
  #scale_y_continuous(              # Fine-tune the x axis.
   # limits = c(0, 200),
    # breaks = c(0, 100, 200) +
  labs(x = "Time (s)", title = "Weightless - Marconi Union", y = "Tempo (BPM)") +
  theme_classic()
  

pureshores <- get_tidy_audio_analysis("5uHnDHx5J48FdT6FjwA7Z8")

tempogramsfalseforpureshores <- pureshores |>
  tempogram(window_size = 4, hop_size = 2, bpms = 80:240, cyclic = FALSE) |>
 ggplot(aes(x = time, y = bpm, fill = power)) +
  geom_raster() +
  scale_fill_viridis_c(guide = "none") +
  labs(x = "Time (s)", title = "Pure Shores", y = "Tempo (BPM)") +
  theme_classic()

tempogramstrueforpureshores <- pureshores |>
  tempogram(window_size = 4, hop_size = 2, bpms = 80:160, cyclic = TRUE) |>
  ggplot(aes(x = time, y = bpm, fill = power)) +
  geom_raster() +
  scale_fill_viridis_c(guide = "none") +
  #scale_y_continuous(              # Fine-tune the x axis.
   # limits = c(0, 200),
    # breaks = c(0, 100, 200) +
  labs(x = "Time (s)", title = "Pure Shores", y = "Tempo (BPM)") +
  theme_classic()
  

jorisvoornexample <- get_tidy_audio_analysis("7fqAVya5281BFkUokiXAAB")

tempogramsfalseforjoris <- jorisvoornexample |>
  tempogram(window_size = 4, hop_size = 2, cyclic = FALSE) |>
  ggplot(aes(x = time, y = bpm, fill = power)) +
  geom_raster() +
  scale_fill_viridis_c(guide = "none") +
  labs(x = "Time (s)", title = "Polydub, Joris Voorn", y = "Tempo (BPM)") +
  theme_classic()

tempogramstrueforjoris <- jorisvoornexample |>
  tempogram(window_size = 4, hop_size = 2, cyclic = TRUE) |>
  ggplot(aes(x = time, y = bpm, fill = power)) +
  geom_raster() +
  scale_fill_viridis_c(guide = "none") +
  labs(x = "Time (s)", title = "Polydub, Joris Voorn", y = "Tempo (BPM)") +
  theme_classic()
grid.arrange(plot1, ncol = 1)
grid.arrange(tempogramsfalseforweightless, tempogramstrueforweightless, tempogramsfalseforpureshores, tempogramstrueforpureshores, tempogramsfalseforjoris, tempogramstrueforjoris, ncol = 2)

```

------------------------------------------------------------------------

Just for fun (and contrast) I had a look at a song from a favorite Techno DJ of mine, Joris Voorn. The song is Polydub from album Four: <https://open.spotify.com/track/7fqAVya5281BFkUokiXAAB?si=44ac45f5d6a94528>

As can been seen very clearly: a typical 125 BPM style song. What also can be seen in the above Tempogram is the effect of 'tempo-octave' where multiple BPM doubles of 125 BPM can be seen in the graph. It is clear to me that Spotify has a much easier job in detecting a clear BPM pattern in a piece of music which is EDM-derived and has clearly articulated (and metered) percussion elements.

------------------------------------------------------------------------

------------------------------------------------------------------------


When looking at this boxplot, it strikes me that only the playlist 'dogs' has a mean around 78 BPM and the other 3 playlists are higher, even the 2 'scientific' ones. Also striking: the mean (and little SD) in the 'Humans' playlist around 120 BPM. I had expected for both science-based lists to have the tempo around 60-80 BPM (for synchronization purposes with humans and dogs rest heartrate) but no such thing; the dogsscience list even has a higher average than the regular dogs list.

When listening to the extreme outliers regarding tempo I can safely assume that the Spotify API isn't working flawlessly: e.g. the 'fastest' song from the dogsscience playlist is "Chorale Prelude No. 5 "Ach bleib bei uns Herr Jesu Christ" in B-Flat Major, BWV 649 (Harp Version)" and analysed at 200 BPM by Spotify: <https://open.spotify.com/track/2Chgj5q0NgvBlFvzYVJvUa?si=5592371b98464c2d>

The same applies to the tempo outlier "Leaves" (<https://open.spotify.com/track/60YvUB0fCvLlTDCc99wpgD?si=5f3c0edf44fd4713>) for the regular dogs playlist, clocked at 204 BPM. When listening it's apparently wrong. It's a piano-only piece of music with clearly some rubato-syle tempo changes.

Initially it was difficult to assess any tempo-related aspects of the song Weightless from tempograms because the lower point of the y-axis was set at 80 BPM whereas Weightless is slower than that. Later on I found out how to set the BPM starting point lower, and set it at 40 BPM.

In addition: I found out (from my own research, i.e. listening to the song on Spotify MANY times and clocking it) that the song Weightless is actually starting a bit slower (namely around 60 BPM) than various sources on the internet (e.g. [here](https://songbpm.com/@marconi-union/weightless)) to be starting, which is around 70 BPM. Also, very interestingly, the Spotify API gives a 71 BPM tempo! Listen yourselves, clearly incorrect!

Irrespective of this, the song is **intendedly** designed to start at a certain BPM and then -after some 5 minutes- slows down towards a lower BPM figure. Reason: assisting in bringing the heartrate of the listener down. Yet, hardly -if at all- to be seen in the tempograms: not the 60 BPM start nor the descend to around 52-54 BPM. The problem relates to the fact that the song **does** have a (low on loudness but definitely consciously hearable) 4/4 meter-type beat but the chords elements are largely ambient-like synths without clear onsets. There **are** melodical elements but they play a minor role and don't necessarily occur in a structured/repetitive manner on anyone of the 4/4 meter beats.

So, I allowed myself a sidestep towards another tool, namely Sonic Visualiser wherein (see blue-pink graph below) the waveform of Weightless is shown together with a green line that shows the result from a transform function (add-on to Sonic Profiler) for tempo and beats. The vertical black line, at 6.02 minutes into the song marks the beginning of the descent of the measured BPM's. Please note: you see here a so-called 'tempo-octave': the green line averages until minute 6.02 around 122 BPM which can be interpreted as double the 'real' (at least perceived by me!)

![](images/sonic.png){width="14cm"}

------------------------------------------------------------------------



